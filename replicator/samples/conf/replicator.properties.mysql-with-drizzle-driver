#################################
# REPLICATOR.PROPERTIES.MYSQL   #
#################################
#
# This file contains properties for MySQL replication.
#
# NOTE TO ALL USERS:  Blank property values are assigned as empty strings. 
# To assign the default value, comment out the key=value assignment.
#
# NOTE TO WINDOWS USERS:  Single backslash characters are treated as escape 
# characters.  You must use forward slash (/) or double backslashes in file 
# names. 

#################################
# GLOBAL REPLICATOR PARAMETERS  #
#################################

# Database connection information. 
replicator.global.db.host=localhost
replicator.global.db.port=3306
replicator.global.db.user=tungsten
replicator.global.db.password=secret

# Properties for extractors only.  This supports direct piplines. 
replicator.global.extract.db.host=${replicator.global.db.host}
replicator.global.extract.db.port=${replicator.global.db.port}
replicator.global.extract.db.user=${replicator.global.db.user}
replicator.global.extract.db.password=${replicator.global.db.password}

# Replicator role.  You must specify a value that corresponds to a 
# pipeline name like master, slave, direct, etc.  There is no default 
# for this value--it must be set or the replicator will not go online.
replicator.role=master

# URI to which we connect when this replicator is a slave.   
replicator.master.connect.uri=thl://masterhost:2112/

# URI for our listener when we are acting as a master.  Slaves 
# use this as their connect URI.
replicator.master.listen.uri=thl://thishost:2112/

# Replicator auto-enable.  If true, replicator automatically goes online 
# at start-up time. 
replicator.auto_enable=true

# Determines whether replication service is started up
# as a thread internal to the ReplicationServiceManager or
# whether it runs in a separate, detached JVM.
replicator.detached=false

# Source ID. This required parameter is used to identify replication
# event source.  It must be unique for each replicator node.
replicator.source_id=localhost

# Site to which the replicator belongs.
site.name=default

# Cluster name to which the replicator belongs.
cluster.name=default

# Replication service type.  Values are 'remote' or 'local'.  Local services 
# do not log updates to Tungsten catalogs.  Remote services do log them. 
replicator.service.type=local

# Name of this replication service.  
service.name=local

# Name of the local replication service.  This parameter must be set when 
# performing bi-directional replication using a remote slave.  Events 
# generated by this service are dropped, thereby preventing replication 
# loops.  It may not be the same as the value of service.name if the service
# is remote. 
local.service.name=local

# Schema to store Replicator catalog tables. 
replicator.schema=tungsten_${service.name}

# Engine used for Replicator catalog tables. If it is undefined or empty, 
# tungsten will use mysql default storage engine. By default,
# it is set to use innodb.
replicator.table.engine=innodb

# Global queue size for pipelines.  This defines the number of events
# buffered between stages.  Values greater than 1 improve performance
# dramatically but mean that you need to have enough heap memory to
# handle blobs and large transaction fragments.
replicator.global.buffer.size=10

# For parallel replication we have a global apply thread count. 1 means
# that apply is single threaded. 
replicator.global.apply.channels=1

# Information regarding the RMI port to use
# for advertising JMX services
replicator.rmi_port=10000

# Policy for shard assignment based on default database.  If 'stringent', use
# default database only if SQL is recognized.  For 'relaxed' always use the 
# default database if it is available. 
replicator.shard.default.db=stringent

# Used by manager to create datasources dynamically
replicator.resourceJdbcUrl=jdbc:mysql://${replicator.global.db.host}:${replicator.global.db.port}/${DBNAME}?jdbcCompliantTruncation=false&zeroDateTimeBehavior=convertToNull&tinyInt1isBit=false&allowMultiQueries=true&yearIsDateType=false
replicator.resourceJdbcDriver=com.mysql.jdbc.Driver
replicator.resourceVendor=mysql
replicator.resourcePrecedence=99
replicator.resourceVendor=mysql
replicator.vipInterface=eth0:0
replicator.vipAddress=192.168.0.1
replicator.resourceLogPattern=mysql-bin
replicator.resourceLogDir=/var/log/mysql
replicator.resourcePort=${replicator.global.db.port}
replicator.resourceDataServerHost=${replicator.global.db.host}

##########################
# OPEN REPLICATOR PLUGIN #
##########################

# Available OpenReplicator providers
replicator.plugin.tungsten=com.continuent.tungsten.replicator.management.tungsten.TungstenPlugin
replicator.plugin.script=com.continuent.tungsten.replicator.management.script.ScriptPlugin

# Chosen OpenReplicator provider
replicator.plugin=tungsten

# Script provider root dir and configuration file. Fill these
# if ScriptPlugin is chosen replicator provider
#
#replicator.script.root_dir=@SCRIPTROOT@
#replicator.script.conf_file=@SCRIPTCONF@
#replicator.script.processor=@SCRIPTFILE@

#################################
# REPLICATOR PIPELINES          #
#################################

# Generic pipelines.
replicator.pipelines=master,slave,direct,dummy

# MASTER PIPELINE:  Two stages with an intervening queue:  extract from binlog
# and place events in THL.
replicator.pipeline.master=binlog-to-q,q-to-thl
replicator.pipeline.master.stores=thl,queue

replicator.stage.binlog-to-q=com.continuent.tungsten.replicator.pipeline.SingleThreadStageTask
replicator.stage.binlog-to-q.extractor=mysql
replicator.stage.binlog-to-q.applier=queue

replicator.stage.q-to-thl=com.continuent.tungsten.replicator.pipeline.SingleThreadStageTask
replicator.stage.q-to-thl.extractor=queue
replicator.stage.q-to-thl.applier=thl-local
replicator.stage.q-to-thl.blockCommitRowCount=${replicator.global.buffer.size}

# SLAVE PIPELINE:  three stages:  extract from remote THL to local THL;
# extract from local THL to queue; apply from queue to DBMS.
replicator.pipeline.slave=remote-to-thl,thl-to-q,q-to-dbms
replicator.pipeline.slave.stores=thl,parallel-queue
replicator.pipeline.slave.syncTHLWithExtractor=false

replicator.stage.remote-to-thl=com.continuent.tungsten.replicator.pipeline.SingleThreadStageTask
replicator.stage.remote-to-thl.extractor=thl-remote
replicator.stage.remote-to-thl.applier=thl-local

replicator.stage.thl-to-q=com.continuent.tungsten.replicator.pipeline.SingleThreadStageTask
replicator.stage.thl-to-q.extractor=thl-local
replicator.stage.thl-to-q.applier=parallel-q-applier
replicator.stage.thl-to-q.blockCommitRowCount=${replicator.global.buffer.size}

replicator.stage.q-to-dbms=com.continuent.tungsten.replicator.pipeline.SingleThreadStageTask
replicator.stage.q-to-dbms.extractor=parallel-q-extractor
replicator.stage.q-to-dbms.applier=mysql
replicator.stage.q-to-dbms.filters=mysqlsessions,bidiSlave
# Use this form to ignore SQL statements from specific MySQL servers.
#replicator.stage.q-to-dbms.filters=mysqlsessions,bidiSlave,ignoreserver
replicator.stage.q-to-dbms.taskCount=${replicator.global.apply.channels}
replicator.stage.q-to-dbms.blockCommitRowCount=${replicator.global.buffer.size}

# DIRECT PIPELINE:  Four stages with intervening queues to move data from 
# master to slave.  Transactions flow from the head extractor to an 
# in-memory queue, to the THL, then to a parallel queue and finally to the 
# database.  For best results you may wish to run on the slave and use 
# relay logs instead of reading files directly on the master and applying 
# over the network to the slave.
replicator.pipeline.direct=d-binlog-to-q,d-q-to-thl,d-thl-to-pq,d-pq-to-dbms
replicator.pipeline.direct.stores=queue,thl,parallel-queue
replicator.pipeline.direct.autoSync=true

# Extract from binlog into queue. 
replicator.stage.d-binlog-to-q=com.continuent.tungsten.replicator.pipeline.SingleThreadStageTask
replicator.stage.d-binlog-to-q.extractor=mysql
replicator.stage.d-binlog-to-q.applier=queue

# Write from queue to Tungsten log. 
replicator.stage.d-q-to-thl=com.continuent.tungsten.replicator.pipeline.SingleThreadStageTask
replicator.stage.d-q-to-thl.extractor=queue
replicator.stage.d-q-to-thl.applier=thl-local

# Extract from Tungsten log to parallel queue. 
replicator.stage.d-thl-to-pq=com.continuent.tungsten.replicator.pipeline.SingleThreadStageTask
replicator.stage.d-thl-to-pq.extractor=thl-local
replicator.stage.d-thl-to-pq.applier=parallel-q-applier
replicator.stage.d-thl-to-pq.blockCommitRowCount=${replicator.global.buffer.size}

# Write from parallel queue to database. 
replicator.stage.d-pq-to-dbms=com.continuent.tungsten.replicator.pipeline.SingleThreadStageTask
replicator.stage.d-pq-to-dbms.extractor=parallel-q-extractor
replicator.stage.d-pq-to-dbms.applier=mysql
replicator.stage.d-pq-to-dbms.filters=mysqlsessions
replicator.stage.d-pq-to-dbms.taskCount=${replicator.global.apply.channels}
replicator.stage.d-pq-to-dbms.blockCommitRowCount=${replicator.global.buffer.size}

# DUMMY PIPELINE:  A single stage to extract events and throw them away.  This
# is useful for testing ability to read the log.
replicator.pipeline.dummy=binlog-to-dummy
replicator.pipeline.dummy.autoSync=true

replicator.stage.binlog-to-dummy=com.continuent.tungsten.replicator.pipeline.SingleThreadStageTask
replicator.stage.binlog-to-dummy.extractor=mysql
replicator.stage.binlog-to-dummy.applier=dummy

#################################
# TRANSACTION HISTORY LOG (THL) #
#################################

# The following parameters set up THL database access.  If you use a
# a database name other than 'tungsten' you must change property
# replicator.schema to match. (JIRA TREP-157). 
# NOTE:  If you run multiple replication services, beware of collisions 
# on log file names and THL lister ports. 

# Set the THL storage implementation class.  For disk logs use commercial class
# com.continuent.tungsten.enterprise.replicator.thl.EnterpriseTHL.  
replicator.store.thl=com.continuent.tungsten.enterprise.replicator.thl.EnterpriseTHL

# Set the storage accessor.  If you use disk logging this must be commercial
# class com.continuent.tungsten.enterprise.replicator.thl.DiskTHLStorage. 
replicator.store.thl.storage=com.continuent.tungsten.enterprise.replicator.thl.DiskTHLStorage

# All THL implementations require database access to store metadata. 
replicator.store.thl.url=jdbc:mysql:thin://${replicator.global.db.host}:${replicator.global.db.port}/tungsten_${service.name}?createDB=true
replicator.store.thl.user=${replicator.global.db.user}
replicator.store.thl.password=${replicator.global.db.password}

# Uncomment the following properties to control disk log storage location
# and size of files.  These are default values.  NOTE:  naming the logs by 
# service name is critical to prevent replication services from overwriting
# other service logs. 
replicator.store.thl.log_dir=${replicator.home.dir}/../cluster-home/logs/${service.name}
replicator.store.thl.log_file_size=100000000

# Change the following property to control the buffer size used for THL
# I/O operations.  The default of 128k seems to work well. 
replicator.store.thl.bufferSize=131072

# The flush interval is the number of milliseconds that log writes may delay
# before being forced to storage.  0 means that every flush call flushes 
# immediately.  Larger values can help buffer writes but add latency to 
# pipelines. 
replicator.store.thl.flushIntervalMillis=0

# Flush operations normally release writes to the OS but do 
# not force them to storage.  You may request a full fsync on each flush by 
# setting the following property to true.  This makes log updates more 
# durable.  If you do this the flush interval should be relatively large to
# to avoid impacting overall throughput. 
replicator.store.thl.fsyncOnFlush=false

# To drop log files after a certain period, set the retention to an interval
# which is <number>{d|h|m|s}, where the letters stand for days, hours, minutes,
# or seconds respectively.  If unset logs are retained indefinitely.
replicator.store.thl.log_file_retention=7d

# The THL serialization for events is pluggable.  The default is Protobuf
# serialization which is relatively fast and compact.  Java serialization
# is also provided but is experimental.
replicator.store.thl.event_serializer=com.continuent.tungsten.enterprise.replicator.thl.serializer.ProtobufSerializer

# The disk log can compute checksums automatically on log records.  This
# is enabled by default but can cut performance by 50% or more.  Set false
# for faster log performance.
replicator.store.thl.doChecksum=false

# The THL storage transfer protocol is pluggable.  Use commercial class 
# com.continuent.tungsten.enterprise.replicator.thl.ConnectorV2 for fast
# byte-based transfer. 
#replicator.thl.protocol=com.continuent.tungsten.replicator.thl.Connector;

# Maximum number of events to transfer at once.  Higher values are better
# but as with queue store sizes require more memory.
replicator.thl.protocol.buffer_size=${replicator.global.buffer.size}

# THL listener address for remote access.  To listen on all ports, 
# use a value like thl://0.0.0.0:2112/.  (2112 is default
# port for Tungsten configurations.)  Note that this URI must provide
# a listener that corresonds to the URL in replicator.master.listen.uri, or 
# slaves may connect to the wrong port. 
replicator.store.thl.storageListenerUri=thl://0.0.0.0:2112

# The following settings can be used in order to get better
# performance.  THL on the slave side is caching events that are
# received from the master THL.  This is done in order to prevent
# from reading events from THL database on the slave side.  The
# size of this cache can be changed by the following setting. By
# default, this cache only contains one event at a time.  
#
# WARNING: increasing the size of the cache will potientially require 
# more memory.

replicator.store.thl.cacheSize=0

# The transfer rate between the master and the slave can be improved
# by setting the following setting to a value greater than 1, which
# is the default.  In doing so, this will also require more memory
# on the master side.

replicator.store.thl.resetPeriod=1

#################################
# IN-MEMORY QUEUE STORE         #
#################################

# In-memory storage to buffer events between stages.
replicator.store.queue=com.continuent.tungsten.replicator.storage.InMemoryQueueStore
replicator.store.queue.maxSize=${replicator.global.buffer.size}

# Parallel queue storage. 
replicator.store.parallel-queue=com.continuent.tungsten.enterprise.replicator.store.ParallelQueueStore
replicator.store.parallel-queue.maxSize=${replicator.global.buffer.size}
replicator.store.parallel-queue.partitions=${replicator.global.apply.channels}
replicator.store.parallel-queue.partitionerClass=com.continuent.tungsten.enterprise.replicator.store.ShardListPartitioner

##############
# EXTRACTORS #
##############

# MySQL binlog extractor properties.  
replicator.extractor.mysql=com.continuent.tungsten.replicator.extractor.mysql.MySQLExtractor
replicator.extractor.mysql.binlog_dir=/var/lib/mysql
replicator.extractor.mysql.binlog_file_pattern=mysql-bin
replicator.extractor.mysql.jdbcHeader=jdbc:mysql:thin://
replicator.extractor.mysql.host=${replicator.global.extract.db.host}
replicator.extractor.mysql.port=${replicator.global.extract.db.port}
replicator.extractor.mysql.user=${replicator.global.extract.db.user}
replicator.extractor.mysql.password=${replicator.global.extract.db.password}
replicator.extractor.mysql.parseStatements=true

# Binlog mode defines whether we are replicating from a MySQL master
# or taking over from the SQL thread on a MySQL slave.  Legal values
# are 'master' or 'relay-slave' respectively.  
replicator.extractor.mysql.binlogMode=master

# Parse statements for binary data.
replicator.extractor.mysql.parseStatements=true

# Use bytes to transfer strings.  This should be set to true when using MySQL
# row replication and the table or column character set differs from the
# character set in use on the server.
replicator.extractor.mysql.usingBytesForString=true

# Fragment size in bytes for transaction splitting.  0 means no splitting
# will occur.  Default is to fragment at 1M bytes.  This keeps under the
# usual limit for things like max_packet_size on MySQL servers.
replicator.extractor.mysql.transaction_frag_size=1000000

# When using relay logs we download from the master into binlog_dir.  This 
# is used for off-board replication. 
replicator.extractor.mysql.useRelayLogs=false

# When you turn on relay logs, you must define a location for them.  This
# overrides the normal log directory.
replicator.extractor.mysql.relayLogDir=${replicator.home.dir}/../cluster-home/relay-logs

# The relay log wait timeout is the number of seconds to wait for the relay
# log position to catch up to the current extract position that Tungsten
# replication wants.  0 is infinite.
replicator.extractor.mysql.relayLogWaitTimeout=0

# The relay log retention is the number of relay logs to keep before deleting
# them automatically.
replicator.extractor.mysql.relayLogRetention=10

# Local THL extractor. 
replicator.extractor.thl-local=com.continuent.tungsten.replicator.thl.THLStoreAdapter
replicator.extractor.thl-local.storeName=thl

# Remote THL extractor. 
replicator.extractor.thl-remote=com.continuent.tungsten.replicator.thl.RemoteTHLExtractor
replicator.extractor.thl-remote.connectUri=${replicator.master.connect.uri}
# If true, check to ensure logs are consistent.  
replicator.extractor.thl-remote.checkSerialization=true

# Queue extractor.
replicator.extractor.queue=com.continuent.tungsten.replicator.storage.InMemoryQueueAdapter
replicator.extractor.queue.storeName=queue

# Parallel queue extractor. 
replicator.extractor.parallel-q-extractor=com.continuent.tungsten.enterprise.replicator.store.ParallelQueueExtractor 
replicator.extractor.parallel-q-extractor.storeName=parallel-queue

# Dummy extractor.
replicator.applier.dummy=com.continuent.tungsten.replicator.extractor.DummyExtractor
replicator.applier.dummy.storeAppliedEvents=false

############
# APPLIERS #
############

# MySQL applier properties.  NOTE:  url_options are extra JDBC options that 
# make the applier behave like a regular client library connection.  For more 
# information check replicator or MySQL Connector/J documentation. 
replicator.applier.mysql=com.continuent.tungsten.replicator.applier.MySQLDrizzleApplier
replicator.applier.mysql.host=${replicator.global.db.host}
replicator.applier.mysql.port=${replicator.global.db.port}
replicator.applier.mysql.user=${replicator.global.db.user}
replicator.applier.mysql.password=${replicator.global.db.password}
# Regular expression to ignore specific session variables in applier.  
# Ignoring autocommit is required in order not to break the applier logic.
replicator.applier.mysql.ignoreSessionVars=autocommit

# Local THL applier. 
replicator.applier.thl-local=com.continuent.tungsten.replicator.thl.THLStoreAdapter
replicator.applier.thl-local.storeName=thl

# Queue applier.
replicator.applier.queue=com.continuent.tungsten.replicator.storage.InMemoryQueueAdapter
replicator.applier.queue.storeName=queue

# Parallel queue applier. 
replicator.applier.parallel-q-applier=com.continuent.tungsten.enterprise.replicator.store.ParallelQueueApplier 
replicator.applier.parallel-q-applier.storeName=parallel-queue

# Dummy applier.
replicator.applier.dummy=com.continuent.tungsten.replicator.applier.DummyApplier

###########
# FILTERS # 
###########

# Dummy filter.  A filter that does nothing. 
replicator.filter.dummy=com.continuent.tungsten.replicator.filter.DummyFilter

# Logging filter.  Logs each event to current system log. 
replicator.filter.logger=com.continuent.tungsten.replicator.filter.LoggingFilter

# Database transform filter.  Transforms database names that match the 
# from_regex are transformed into the to_regex.  
replicator.filter.dbtransform=com.continuent.tungsten.replicator.filter.DatabaseTransformFilter
replicator.filter.dbtransform.from_regex=foo
replicator.filter.dbtransform.to_regex=bar

# Adds statements required to replicate temporary tables and session variables
# correctly within a session context
replicator.filter.mysqlsessions=com.continuent.tungsten.replicator.filter.MySQLSessionSupportFilter

# Transforms database, table and column names into upper or lower case. In case
# of statement replication generally it transforms everything except quoted
# string values.
replicator.filter.casetransform=com.continuent.tungsten.replicator.filter.CaseMappingFilter
replicator.filter.casetransform.to_upper_case=true

# JavaScript call out filter. Calls script prepare(), filter(event) and
# release() functions. Define multiple filters with different names in case you
# need to call more than one script.
replicator.filter.javascript=com.continuent.tungsten.replicator.filter.JavaScriptFilter
replicator.filter.javascript.script=${replicator.home.dir}/samples/extensions/javascript/filter.js
replicator.filter.javascript.sample_custom_property="Sample value"

# Filter to be used on slaves For every
# CREATE|DROP|ALTER|RENAME TABLE event adds a "SET foreign_key_checks=0"
# statement.
replicator.filter.foreignkeychecks=com.continuent.tungsten.replicator.filter.JavaScriptFilter 
replicator.filter.foreignkeychecks.script=${replicator.home.dir}/samples/extensions/javascript/foreignkeychecks.js 

# Time delay filter.  Should only be used on slaves, as it delays storage
# of new events on the master.  The time delay is in seconds. 
replicator.filter.delay=com.continuent.tungsten.replicator.filter.TimeDelayFilter
replicator.filter.delay.delay=300

# Remote slave filter.  This filter sanitizes events on remote slaves by 
# dropping events produced on the same service ID.  To use this you *must* 
# set the localServiceName parameter, which must be the same as the 
# service.name parameter of the local service that reads the binlog. 
replicator.filter.bidiSlave=com.continuent.tungsten.enterprise.replicator.filter.BidiRemoteSlaveFilter
replicator.filter.bidiSlave.localServiceName=${local.service.name}

# If true allow statements that may be unsafe for bi-directional replication. 
replicator.filter.bidiSlave.allowBidiUnsafe=false

# If true allow updates from all other remote services, not just this one. 
replicator.filter.bidiSlave.allowAnyRemoteService=false

# Server ID filter.  This filter drops SQL statements from one or more MySQL
# server IDs, which are given as a comma-separated list.
replicator.filter.ignoreserver=com.continuent.tungsten.replicator.filter.JavaScriptFilter
replicator.filter.ignoreserver.script=${replicator.home.dir}/samples/scripts/javascript-advanced/ignore_server_id.js
replicator.filter.ignoreserver.server_ids=3,7

################################
# BACKUP/RESTORE CONFIGURATION #
################################

# List of configured backup agents.  Uncomment appropriately for your site. 
#replicator.backup.agents=mysqldump,lvm,script

# Default backup agent.
replicator.backup.default=mysqldump

# MySqlDump Agent--backup using mysql dump utility; restore with mysql.
replicator.backup.agent.mysqldump=com.continuent.tungsten.replicator.backup.mysql.MySqlDumpAgent
replicator.backup.agent.mysqldump.host=${replicator.global.db.host}
replicator.backup.agent.mysqldump.port=${replicator.global.db.port}
replicator.backup.agent.mysqldump.user=${replicator.global.db.user}
replicator.backup.agent.mysqldump.password=${replicator.global.db.password}
replicator.backup.agent.mysqldump.dumpDir=/tmp

# Options for MySQL 5.1 with BUG 34306 (found in versions up to 5.1.34).
replicator.backup.agent.mysqldump.mysqldumpOptions=--all-databases --add-drop-database --skip-lock-tables --skip-add-locks
replicator.backup.agent.mysqldump.hotBackupEnabled=false

# Options for MySQL 5.0 or 5.1 with BUG 34306 fix.
#replicator.backup.agent.mysqldump.mysqldumpOptions=--all-databases --add-drop-database
#replicator.backup.agent.mysqldump.hotBackupEnabled=true

# Options to backup a few databases only.  This works on any database and 
# can run hot on slaves thanks to the --single-transation option. 
#replicator.backup.agent.mysqldump.mysqldumpOptions=--databases --add-drop-database --single-transaction tungsten test
#replicator.backup.agent.mysqldump.hotBackupEnabled=true

# LVM Snapshot Agent--backup using LVM snapshot and file copy
replicator.backup.agent.lvm=com.continuent.tungsten.replicator.backup.mysql.MySqlLvmDumpAgent
replicator.backup.agent.lvm.host=localhost
replicator.backup.agent.lvm.port=${replicator.global.db.port}
replicator.backup.agent.lvm.user=${replicator.global.db.user}
replicator.backup.agent.lvm.password=${replicator.global.db.password}
replicator.backup.agent.lvm.dumpDir=/tmp
replicator.backup.agent.lvm.volumeGroup=VolGroup00
replicator.backup.agent.lvm.logicalVolume=LogVol00
replicator.backup.agent.lvm.logicalVolumeMount=/
replicator.backup.agent.lvm.dataDir=/var/lib/mysql
replicator.backup.agent.lvm.snapshotName=mysqlsnap
replicator.backup.agent.lvm.snapshotSize=2G
replicator.backup.agent.lvm.snapshotMount=/mnt/snapshots
replicator.backup.agent.lvm.commandPrefix=sudo
replicator.backup.agent.lvm.mysqlStart=/etc/init.d/mysqld start
replicator.backup.agent.lvm.mysqlStop=/etc/init.d/mysqld stop

# Script Agent--Executes a script to backup or restore. 
replicator.backup.agent.script=com.continuent.tungsten.replicator.backup.generic.ScriptDumpAgent
replicator.backup.agent.script.script=${replicator.home.dir}/samples/scripts/backup/custom-backup.sh
replicator.backup.agent.script.commandPrefix=
replicator.backup.agent.script.hotBackupEnabled=false

# Xtrabackup Agent--Executes a script that uses xtrabackup to backup or restore. 
replicator.backup.agent.xtrabackup=com.continuent.tungsten.replicator.backup.generic.ScriptDumpAgent
replicator.backup.agent.xtrabackup.script=${replicator.home.dir}/samples/scripts/backup/xtrabackup.sh
replicator.backup.agent.xtrabackup.commandPrefix=sudo
replicator.backup.agent.xtrabackup.hotBackupEnabled=true
# Xtrabackup can handle the following options
#   user            - The mysql user to use during backup [default: root]
#   password        - The password for the mysql user [default: ]
#		host						- The hostname for the database server [default: localhost]
#		port						- The port for the database server [default: 3306]
#   directory       - A working directory to stage backup files in [default: /tmp/innobackup]
#   archive         - A non-existing file that will be created to package the backup files [default: /tmp/innobackup.tar]
#   service         - The name of the mysql service [default: mysql]
#   mysqldatadir    - The absolute path for the mysql data directory [default: /var/lib/mysql]
#   mysqluser       - The os user that mysql runs as [default: mysql]
#   mysqlgroup      - The os group that mysql runs as [default: mysql]
#		mysql_service_comand	- The command to call when stopping/starting MySQL [default: /etc/init.d/mysql]
# replicator.backup.agent.xtrabackup.options=user=tungsten&password=secret&directory=/tmp/backup
replicator.backup.agent.xtrabackup.options=user=${replicator.global.db.user}&password=${replicator.global.db.password}

# List of configured storage agents.  Uncomment appropriately for your site. 
#replicator.storage.agents=fs

# Default storage agent.
replicator.storage.default=fs

# File system storage agent.  For best results the directory parameter should
# be a shared file system visible to all replicators.  NOTE: CRC file checking
# may be time-consuming for large files; it is recommended if you can afford
# to check.  (Who really wants to load a bad backup??)
replicator.storage.agent.fs=com.continuent.tungsten.replicator.backup.FileSystemStorageAgent
replicator.storage.agent.fs.directory=/mnt/backups
replicator.storage.agent.fs.retention=3
replicator.storage.agent.fs.crcCheckingEnabled=true

####################################################
# ERROR-HANDLING AND CONSISTENCY-CHECKING POLICIES #
####################################################

# How to react on extractor failure. Possible values are 'stop' or 'warn'. 
replicator.extractor.failure_policy=stop

# How to react on applier failure. Possible values are 'stop' or 'warn'. 
replicator.applier.failure_policy=stop

# Event checksum algorithm.  Possible values are 'crc' or 'none'.
replicator.event.checksum=none

# How to react on consistency check failure.  Possible values are 'stop' or 
# 'warn'. 
replicator.applier.consistency_policy=stop

# Should consistency check be sensitive to column names and/or types? Settings
# on a slave must be identical to master's. Values are 'true' or 'false'. 
replicator.applier.consistency_column_names=true
# TUC-20:  Must disable column type checking when going from
# Connector/J on master to drizzle on slave.
replicator.applier.consistency_column_types=false
